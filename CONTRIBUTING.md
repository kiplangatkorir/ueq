# Contributing to UEQ

First of all, thank you for your interest in contributing to **UEQ (Uncertainty Estimation & Quantification)**.

UEQ is an open-source project focused on **practical, production-ready, and research-aligned uncertainty quantification for machine learning**. Contributions are welcome from ML engineers, researchers, and motivated students.

This document explains **how to get started, what kinds of contributions we are looking for, and how to work effectively with the project**.

## What This Is (and Is Not)

**UEQ contributions are:**

* Open-source and public
* Self-directed (no interviews, no applications)
* Focused on real ML uncertainty problems
* Reviewed for correctness and clarity

**UEQ contributions are NOT:**

* A paid job (at least for now)
* A coursework dumping ground
* A resume submission process

If this works for you, you are very welcome here.

## How to Start (Recommended Path)

1. **Star the repository** (helps visibility)
2. **Read the README** to understand UEQ’s goals
3. **Browse the Issues**
4. Pick one of:

   * `good-first-issue`
   * `core`
   * `research`
5. Comment on the issue to signal you are working on it
6. Open a Pull Request when ready

That’s it — no permission required.

## Types of Contributions We Value

### 1. Core Features

* Uncertainty quantification methods
* Conformal prediction variants
* Drift-aware recalibration
* Evaluation metrics

### 2. Research & Experimental Work

* Evidential uncertainty
* Bayesian/approximate Bayesian methods
* Continual & online uncertainty
* Time-series and structured outputs

Research contributions **do not need to be perfect**, but they must be:

* Clearly documented
* Empirically evaluated
* Honest about limitations

### 3. Benchmarks & Datasets

* Synthetic datasets with known uncertainty
* Real-world benchmarks
* Reproducible evaluation protocols

### 4. Documentation & Examples

* Tutorials
* Example notebooks
* Visualization utilities

## Issue Labels Explained

* `good-first-issue` – Well-scoped, beginner-friendly
* `core` – Planned for v1.0.x releases
* `research` – Exploratory/experimental
* `benchmark` – Evaluation & datasets
* `architecture` – Design & extensibility
* `rfc` – Requires discussion before implementation

Please ensure that you respect the intent of each label.

## Coding Guidelines

* Follow existing project structure
* Write clear, readable Python
* Prefer explicit over clever
* Add docstrings for public APIs
* Add tests where reasonable

Statistical correctness > performance > convenience.

## Pull Request Guidelines

A good PR:

* References an existing issue
* Explains *what* was done and *why*
* Includes tests or validation (where applicable)
* Updates documentation if behavior changes

Draft PRs are welcome.

## Review Process

* Maintainers will review for correctness and clarity
* Feedback may be technical and direct
* Iteration is expected — this is normal

The goal is **quality and long-term maintainability**, not speed.

## Community & Conduct

Be respectful, professional, and constructive.

This project follows a simple rule:

> Be rigorous with ideas, kind with people.

Harassment, plagiarism, or bad-faith contributions will not be tolerated.

## Recognition

Contributors will be:

* Credited in release notes
* Listed in the repository
* Acknowledged in documentation where appropriate

Consistent contributors may be invited to become maintainers.

## Questions & Discussion

* Use GitHub Issues for technical discussion
* Open RFC issues for design proposals
* Discord may be created once the community grows

If you are unsure where to start, open an issue and ask.

**Thank you for helping build trustworthy machine learning.**
